{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"BQ Analysis v1_1.xlsx\", \"Project BQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(str)\n",
    "df[\"PREPROCESS_TEXT\"] = df[\"BQ_TRADE\"] + \",\" + df[\"BQ_HEADING\"] + \",\" + df[\"BQ_SUB_HEADING\"] + \",\" + df[\"BQ_ITEM_HEADING\"] + \",\" + df[\"BQ_ITEM_DESCRIPTION\"] + \",\" + df[\"AC_TRADE_CODE\"]\n",
    "#df['RESULT'] = \"\"\n",
    "#training_df = df.loc[df['CONTRACT_CODE'] == \"FL49\"].copy() // not enough training data?\n",
    "training_df = df[df['CONTRACT_CODE'] != \"QHP3\"]\n",
    "test_df = df[df['CONTRACT_CODE'] == \"QHP3\"]\n",
    "\n",
    "df[\"PREPROCESS_TEXT\"] = list(map(lambda x: x.lower(), list(df[\"PREPROCESS_TEXT\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_labels = tuple(training_df[\"AC_TRADE_CODE\"])\n",
    "training_texts = tuple(training_df[\"PREPROCESS_TEXT\"])\n",
    "#print(training_labels)\n",
    "#print(training_text)\n",
    "\n",
    "\n",
    "\n",
    "#test_labels = tuple(test_df[\"RESULT\"])\n",
    "test_texts = tuple(test_df[\"PREPROCESS_TEXT\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AP       0.00      0.00      0.00         0\n",
      "          AR       0.93      1.00      0.96       215\n",
      "          AS       1.00      0.62      0.77        45\n",
      "          BR       1.00      1.00      1.00        35\n",
      "          BS       0.00      0.00      0.00         0\n",
      "          CJ       0.99      1.00      1.00       368\n",
      "          DA       0.00      0.00      0.00        12\n",
      "          DR       1.00      0.95      0.98       479\n",
      "          EL       0.97      0.74      0.84      1426\n",
      "          EX       0.22      1.00      0.36        10\n",
      "          FD       0.00      0.00      0.00         0\n",
      "          GL       1.00      1.00      1.00        95\n",
      "          IR       1.00      1.00      1.00        24\n",
      "          PA       0.95      0.99      0.97       427\n",
      "          PL       0.94      1.00      0.97       853\n",
      "          PR       1.00      1.00      1.00        58\n",
      "          PS       0.98      1.00      0.99      2788\n",
      "          SM       0.98      1.00      0.99      2258\n",
      "          SO       1.00      0.56      0.72        82\n",
      "          ST       0.96      1.00      0.98      2712\n",
      "          SU       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96     11887\n",
      "   macro avg       0.71      0.71      0.69     11887\n",
      "weighted avg       0.97      0.96      0.96     11887\n",
      "\n",
      "Accuracy: 0.9615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frankielau\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\frankielau\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "#vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(training_texts)\n",
    "y = training_labels\n",
    "\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "#y_test = test_labels\n",
    "\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "annotated_test_data = list(zip(test_predictions, test_texts))\n",
    "\n",
    "#y_test = np.array(test_labels)\n",
    "print(metrics.classification_report(test_df[\"AC_TRADE_CODE\"], test_predictions))\n",
    "print(\"Accuracy: %0.4f\" % metrics.accuracy_score(test_df[\"AC_TRADE_CODE\"], test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0b8584eff0e9>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['RESULT'], test_df['PROCESS_TEXT'] = zip(*annotated_test_data)\n",
      "<ipython-input-6-0b8584eff0e9>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[\"COMPARE\"] = np.where(test_df[\"AC_TRADE_CODE\"] == test_df[\"RESULT\"], True, False)\n"
     ]
    }
   ],
   "source": [
    "test_df['RESULT'], test_df['PROCESS_TEXT'] = zip(*annotated_test_data)\n",
    "test_df[\"COMPARE\"] = np.where(test_df[\"AC_TRADE_CODE\"] == test_df[\"RESULT\"], True, False)\n",
    "#test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     11429\n",
       "False      458\n",
       "Name: COMPARE, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['COMPARE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model\n",
    "with open('text_vectorizer', 'wb') as picklefile:\n",
    "    pickle.dump(vectorizer,picklefile)\n",
    "\n",
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(clf,picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
